{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "### form Artificial Intelligence: A Modern Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Decision Tree\n",
    "* ### Represents a function that takes a vector of attribute values and returns a “decision,” a single output value. \n",
    " * ### We will focus on Boolean classification.\n",
    " * ### Performs a series of tests, where each node in the tree corresponds to a test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Will Wait?\n",
    "* ### You’re at a restaurant, will you wait for a table?\n",
    "* ### Relevant features (These go into the input vector for each example in the sample):\n",
    "    * ### Alternate: Another restaurant nearby?\n",
    "    * ### Bar: Do they have a bar?\n",
    "    * ### Fri/Sat: true on Fri and Sat\n",
    "    * ### Hungry: Are we hungry?\n",
    "    * ### Patrons: {None, Some, Full}\n",
    "    * ### Price: {$, $$, $$$}\n",
    "    * ### Raining: Is it raining?\n",
    "    * ### Reservation: Did we make a reservation\n",
    "    * ### Type: {French, Italian, Thai, or burger}\n",
    "    * ### WaitEstimate: {0-10 min, 11-30, 31-60, >60}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of learning a decision tree for the above problem, using a sample of labeled data\n",
    "![](imgs/example_tree)\n",
    "* ## Each of the features or attributes above has several different values: Bar is an attribute with the values \"yes\" and \"no\"\n",
    "* ## We pick an attribute, and for each \n",
    "* ## We then assign the examples to boxes according to which value was present in their case: this is splitting\n",
    "* ## Next, we take another attribute, and split the examples according to which value each of the examples had into further boxes, then do more splitting, and so on, until all the remaining boxes have either only those who waited, or those who stayed. Then we can use the tree to predict whether a person outside of our data set will stay or leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some sample data. Customers, with whether or not he/she waited(goal)\n",
    "![Note: the numbers refer to example number in the table shown above](imgs/decision_tree_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## However, the order of splitting attributes is important, splitting on one attribute first instead of another might require more tests be done to separate the examples.\n",
    "* ## Below reflects a poor choice and a good choice regarding which attribute to use for splitting first: splitting on Type yields no new information, but splitting on Patrons brings us closer to separating who waits and who leaves\n",
    "![](imgs/splitting.png)\n",
    "Note: the numbers refer to the sample number in the table above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, what's the best order for splitting, then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desideratum: Find the Smallest Tree\n",
    "* ### Can’t crunch through 2 to the 2 to the n possible trees and see which is smallest!\n",
    "* ### Use the DECISION-TREE-LEARNING algorithm.\n",
    "    * ### Uses the most-important attribute: the one that makes the most difference to the \n",
    "    * ### classification of an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing is a recursive algorithm: after each test there is a remaining mini-tree to go through.\n",
    "* ## Four cases:\n",
    "    * ## The remaining examples are either all positive or negative (stop).\n",
    "    * ## Some pos and neg examples: choose the best attribute left to split on.\n",
    "    * ## No examples left, use plurality classification applied to parent.\n",
    "    * ## No attributes left, both pos and neg examples, use plurality classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Here's the algorithm in pseudocode:\n",
    "\n",
    "![](imgs/dtalgorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tree that results from the above algorithm applied to our sample data\n",
    "\n",
    "![](imgs/tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance Function: Deciding Which Class is Most Important\n",
    "\n",
    "## We want to maximize information gain as much as we can as we create the tree\n",
    "\n",
    "## Entropy: a measure of teh amount of uncertainty of a random variable\n",
    "\n",
    "* ## Expressed in bits: coin tosses\n",
    "    * ## 1 bit: an answer to a yes or no question\n",
    "* ## Information gain = entropy loss\n",
    "\n",
    "## Consider sending messages between treehouses using a pluck string that, per each half a second, it's either pluck or no-pluck\n",
    "\n",
    "![](imgs/pluck.jpg)\n",
    "* ## To send a series of coin tosses, how many single pluck/no-pluck's must we send?\n",
    "    * ## You tell me!\n",
    "* ## To transmit a single letter of the alphabet, how many pluck/no-pluck's?\n",
    "    * ## How many yes-no questions must we answer?\n",
    "    * ## Use binary search for the alphabet\n",
    "\n",
    "![](imgs/scrabble.jpg)\n",
    "    \n",
    "* ## The answer then is:\n",
    "    $$ log_{2}(26)$$\n",
    "* ## How about a poker hand taken from an infinite deck of 52 different cards? You tell me.\n",
    "* ## From now on, we'll use bits instead of pluck/no-pluck's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Entropy: \n",
    "## $$H(V) = \\sum_{k} P(v_{k})log_{2}\\frac{1}{P(v_{k})} = - \\sum_{k} P(v_{k})log_{2}P(v_{k})$$\n",
    "## where V is a random variable with values $v_{k}$ each with probability $P(v_{k})$\n",
    "\n",
    "## Where does that probability come from: the number of times it occurs over the total nuber of occurances (if your sample has 10 in it, and 4 are orange, then the probability of orange is 0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Gain: The initial entropy - the weighted average of the entropy of the child nodes\n",
    "\n",
    "## Suppose we decide to split on attribute $A$ (whatever that is) first. If $A$ has $d$ values $E_{1}, E_{2}, E_{d}$, then it divides the training set into $d$ subsets. \n",
    "\n",
    "## E.g., from the will wait problem: Type, an attribute, has 4 values: French, Thai, Burger, Italian. Thus it divides samples into 4 ($d$) subsets. \n",
    "\n",
    "\n",
    "## Each subset has $p_{k}$ positive examples and $n_{k}$ negative examples, while the total number of positive and negative examples in A is given by $p+n$. The proportion of examples in any given E compared to A is $\\frac{p_{k}+n_{k}}{p+n}$. Then:\n",
    "\n",
    "## $$Remainder(A) = \\sum_{k=1}^{d}\\frac{p_{k}+n_{k}}{p+n}(Entropy(E_{k}))$$\n",
    "\n",
    "\n",
    "## Then the information gain from testing on A is:\n",
    "\n",
    "## $$Gain(A) = Entropy(Parent) - Remainder(A)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Idea: Implement the IMPORTANCE function in terms of entropy loss/information gain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "* ## Randomly split up your training data into subsets. \n",
    "* ## Choose a subset of attributes.\n",
    "* ## Train several trees using these sets.\n",
    "* ## Make predictions using all of the trees, take the prediction with the highest number of votes.\n",
    "\n",
    "![](imgs/random_forests.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try out a decision tree using sci-kit learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree #see http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get our data: we'll use the iris data set for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test sets\n",
    "\n",
    "## The reason we do this is that sometimes our classifier will fit the data so well that it reflects the ideosyncracies of our sample, and not generalize well. This is called overfitting. To get a good idea of how the classifier would do on data it hasn't seen, we reserve some data that we will not use for training and test it out on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\") #see http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To see how we did, test on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy on training data:', 1.0)\n",
      "('Accuracy on test data:', 0.97999999999999998)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "acc_train = clf.score(X_train, y_train)\n",
    "acc_test = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on training data:\", acc_train)\n",
    "print(\"Accuracy on test data:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out a different min samples split and see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy with min split = 2:', 0.97999999999999998)\n",
      "('Accuracy with min split = 50:', 0.97999999999999998)\n"
     ]
    }
   ],
   "source": [
    "clf1 = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=2)\n",
    "clf2 = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=50)\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "acc_min_samples_split_2 = clf1.score(X_test, y_test)\n",
    "acc_min_samples_split_50 = clf2.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy with min split = 2:\", acc_min_samples_split_2)\n",
    "print(\"Accuracy with min split = 50:\", acc_min_samples_split_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.export_graphviz(clf, out_file='imgs/tree.dot')\n",
    "\n",
    "! dot -Tpng imgs/tree.dot -o imgs/result.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  1.  0.  1.  1.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  1.  0.  1.  0.  1.  0.  1.  0.  0.]\n",
      " [ 0.  1.  1.  0.  1.  0.  1.  0.  1.  1.  0.]\n",
      " [ 0.  1.  1.  0.  1.  0.  0.  0.  0.  3.  0.]\n",
      " [ 0.  1.  1.  0.  1.  1.  0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "my_data =  np.loadtxt('willwait_data.csv', dtype=str, delimiter=',')\n",
    "\n",
    "#Need to convert everything to numerical values\n",
    "my_data[my_data=='No'] = 0\n",
    "my_data[my_data =='None'] = 0\n",
    "my_data[my_data=='$'] = 0\n",
    "my_data[my_data=='Mexican'] = 0\n",
    "my_data[my_data=='10'] = 0\n",
    "\n",
    "my_data[my_data=='Yes'] = 1\n",
    "my_data[my_data=='Some'] = 1\n",
    "my_data[my_data=='$$'] = 1\n",
    "my_data[my_data=='Thai'] = 1\n",
    "my_data[my_data=='30'] = 1\n",
    "\n",
    "my_data[my_data=='Full'] = 2\n",
    "my_data[my_data=='$$$'] = 2\n",
    "my_data[my_data=='Burger'] = 2\n",
    "my_data[my_data=='60'] = 2\n",
    "\n",
    "my_data[my_data=='Italian'] = 3\n",
    "\n",
    "my_data = np.delete(my_data, 0, 1) \n",
    "my_data = np.delete(my_data, 0, 0).astype('float64') \n",
    "\n",
    "print(my_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.\n",
      "  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "restaurant_y = my_data[:, 0]\n",
    "print(restaurant_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  0.  1.  1.  0.  0.  1.  0.  0.]\n",
      " [ 1.  1.  0.  1.  0.  1.  0.  1.  0.  0.]\n",
      " [ 1.  1.  0.  1.  0.  1.  0.  1.  1.  0.]\n",
      " [ 1.  1.  0.  1.  0.  0.  0.  0.  3.  0.]\n",
      " [ 1.  1.  0.  1.  1.  0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "restaurant_X = my_data[:, 1:]\n",
    "print(restaurant_X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfr = tree.DecisionTreeClassifier(criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(restaurant_X, restaurant_y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfr.fit(Xr_train, yr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy on training data:', 1.0)\n",
      "('Accuracy on test data:', 0.76923076923076927)\n"
     ]
    }
   ],
   "source": [
    "accr_train = clfr.score(Xr_train, yr_train)\n",
    "accr_test = clfr.score(Xr_test, yr_test)\n",
    "print(\"Accuracy on training data:\", accr_train)\n",
    "print(\"Accuracy on test data:\", accr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.export_graphviz(clfr, out_file='imgs/treer.dot')\n",
    "\n",
    "! dot -Tpng imgs/treer.dot -o imgs/resultr.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/resultr.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
